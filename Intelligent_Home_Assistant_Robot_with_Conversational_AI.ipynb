{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNd+5L/DS7+DM4p/aMoR08T",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MengqinShen/Computer-Vision-with-openCV/blob/main/Intelligent_Home_Assistant_Robot_with_Conversational_AI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ER1EQlkvXiGG"
      },
      "outputs": [],
      "source": [
        "# Intelligent Home Assistant Robot with Conversational AI\n",
        "\n",
        "## Project Overview\n",
        "\n",
        "This project develops an autonomous home assistant robot that integrates advanced conversational AI capabilities with ROS2 (Robot Operating System 2) for distributed system architecture and real-time communication. The robot can navigate autonomously, respond to voice commands, control smart home devices, and provide intelligent assistance.\n",
        "\n",
        "## System Architecture\n",
        "\n",
        "### Core Components\n",
        "- **ROS2 Framework**: Distributed system architecture with real-time communication\n",
        "- **Conversational AI Engine**: Natural language processing and generation\n",
        "- **Navigation System**: SLAM, path planning, and obstacle avoidance\n",
        "- **Smart Home Integration**: IoT device control and automation\n",
        "- **Voice Interface**: Speech recognition and text-to-speech\n",
        "- **Computer Vision**: Object recognition and scene understanding\n",
        "\n",
        "## Hardware Requirements\n",
        "\n",
        "### Minimum Setup\n",
        "- **Computing Platform**: NVIDIA Jetson Xavier NX or Raspberry Pi 4B (8GB)\n",
        "- **LiDAR**: RPLiDAR A1M8 or similar\n",
        "- **Camera**: Intel RealSense D435i depth camera\n",
        "- **Microphone Array**: ReSpeaker 4-Mic Array\n",
        "- **Speaker**: Portable Bluetooth speaker\n",
        "- **Motors**: Differential drive system with encoders\n",
        "- **IMU**: 9-DOF inertial measurement unit\n",
        "- **Wi-Fi Module**: For smart home connectivity\n",
        "\n",
        "### Enhanced Setup\n",
        "- **Computing**: NVIDIA Jetson AGX Orin\n",
        "- **LiDAR**: Velodyne VLP-16 or Ouster OS1-64\n",
        "- **Cameras**: Multiple RGB-D cameras for 360° vision\n",
        "- **Manipulator**: 6-DOF robotic arm (optional)\n",
        "- **Touch Display**: For visual interaction\n",
        "\n",
        "## Software Stack\n",
        "\n",
        "### ROS2 Packages and Dependencies\n",
        "\n",
        "```bash\n",
        "# Core ROS2 packages\n",
        "sudo apt update\n",
        "sudo apt install ros-humble-desktop-full\n",
        "sudo apt install ros-humble-navigation2\n",
        "sudo apt install ros-humble-nav2-bringup\n",
        "sudo apt install ros-humble-slam-toolbox\n",
        "sudo apt install ros-humble-robot-localization\n",
        "sudo apt install ros-humble-joint-state-publisher\n",
        "sudo apt install ros-humble-robot-state-publisher\n",
        "\n",
        "# Computer Vision\n",
        "sudo apt install ros-humble-vision-msgs\n",
        "sudo apt install ros-humble-image-transport\n",
        "sudo apt install ros-humble-cv-bridge\n",
        "\n",
        "# Audio Processing\n",
        "sudo apt install portaudio19-dev\n",
        "sudo apt install espeak espeak-data\n",
        "pip install pyaudio speechrecognition pyttsx3\n",
        "\n",
        "# AI and ML Libraries\n",
        "pip install openai anthropic\n",
        "pip install torch torchvision torchaudio\n",
        "pip install transformers\n",
        "pip install rasa\n",
        "```\n",
        "\n",
        "### ROS2 Package Structure\n",
        "\n",
        "```\n",
        "home_assistant_robot/\n",
        "├── src/\n",
        "│   ├── ha_robot_bringup/\n",
        "│   ├── ha_robot_description/\n",
        "│   ├── ha_robot_navigation/\n",
        "│   ├── ha_robot_ai/\n",
        "│   ├── ha_robot_voice/\n",
        "│   ├── ha_robot_vision/\n",
        "│   └── ha_robot_smart_home/\n",
        "├── launch/\n",
        "├── config/\n",
        "└── maps/\n",
        "```\n",
        "\n",
        "## Implementation\n",
        "\n",
        "### 1. Robot Description (URDF)\n",
        "\n",
        "```xml\n",
        "<!-- ha_robot_description/urdf/robot.urdf.xacro -->\n",
        "<?xml version=\"1.0\"?>\n",
        "<robot xmlns:xacro=\"http://www.ros.org/wiki/xacro\" name=\"home_assistant_robot\">\n",
        "\n",
        "  <!-- Base Properties -->\n",
        "  <xacro:property name=\"base_width\" value=\"0.35\"/>\n",
        "  <xacro:property name=\"base_length\" value=\"0.4\"/>\n",
        "  <xacro:property name=\"base_height\" value=\"0.1\"/>\n",
        "  <xacro:property name=\"wheel_radius\" value=\"0.08\"/>\n",
        "  <xacro:property name=\"wheel_width\" value=\"0.04\"/>\n",
        "\n",
        "  <!-- Base Link -->\n",
        "  <link name=\"base_link\">\n",
        "    <visual>\n",
        "      <geometry>\n",
        "        <box size=\"${base_length} ${base_width} ${base_height}\"/>\n",
        "      </geometry>\n",
        "      <material name=\"blue\">\n",
        "        <color rgba=\"0.2 0.2 0.8 1.0\"/>\n",
        "      </material>\n",
        "    </visual>\n",
        "    <collision>\n",
        "      <geometry>\n",
        "        <box size=\"${base_length} ${base_width} ${base_height}\"/>\n",
        "      </geometry>\n",
        "    </collision>\n",
        "    <inertial>\n",
        "      <mass value=\"15.0\"/>\n",
        "      <inertia ixx=\"0.5\" iyy=\"0.5\" izz=\"0.5\" ixy=\"0\" ixz=\"0\" iyz=\"0\"/>\n",
        "    </inertial>\n",
        "  </link>\n",
        "\n",
        "  <!-- Wheels -->\n",
        "  <xacro:macro name=\"wheel\" params=\"prefix y_reflect\">\n",
        "    <link name=\"${prefix}_wheel\">\n",
        "      <visual>\n",
        "        <geometry>\n",
        "          <cylinder radius=\"${wheel_radius}\" length=\"${wheel_width}\"/>\n",
        "        </geometry>\n",
        "        <material name=\"black\">\n",
        "          <color rgba=\"0.1 0.1 0.1 1.0\"/>\n",
        "        </material>\n",
        "      </visual>\n",
        "      <collision>\n",
        "        <geometry>\n",
        "          <cylinder radius=\"${wheel_radius}\" length=\"${wheel_width}\"/>\n",
        "        </geometry>\n",
        "      </collision>\n",
        "      <inertial>\n",
        "        <mass value=\"2.0\"/>\n",
        "        <inertia ixx=\"0.1\" iyy=\"0.1\" izz=\"0.1\" ixy=\"0\" ixz=\"0\" iyz=\"0\"/>\n",
        "      </inertial>\n",
        "    </link>\n",
        "\n",
        "    <joint name=\"${prefix}_wheel_joint\" type=\"continuous\">\n",
        "      <parent link=\"base_link\"/>\n",
        "      <child link=\"${prefix}_wheel\"/>\n",
        "      <origin xyz=\"0 ${y_reflect*base_width/2} 0\" rpy=\"-1.57 0 0\"/>\n",
        "      <axis xyz=\"0 0 1\"/>\n",
        "    </joint>\n",
        "  </xacro:macro>\n",
        "\n",
        "  <xacro:wheel prefix=\"left\" y_reflect=\"1\"/>\n",
        "  <xacro:wheel prefix=\"right\" y_reflect=\"-1\"/>\n",
        "\n",
        "  <!-- Sensors -->\n",
        "  <link name=\"lidar_link\">\n",
        "    <visual>\n",
        "      <geometry>\n",
        "        <cylinder radius=\"0.05\" length=\"0.03\"/>\n",
        "      </geometry>\n",
        "    </visual>\n",
        "  </link>\n",
        "\n",
        "  <joint name=\"lidar_joint\" type=\"fixed\">\n",
        "    <parent link=\"base_link\"/>\n",
        "    <child link=\"lidar_link\"/>\n",
        "    <origin xyz=\"0.1 0 0.3\" rpy=\"0 0 0\"/>\n",
        "  </joint>\n",
        "\n",
        "</robot>\n",
        "```\n",
        "\n",
        "### 2. Navigation Configuration\n",
        "\n",
        "```yaml\n",
        "# config/nav2_params.yaml\n",
        "amcl:\n",
        "  ros__parameters:\n",
        "    use_sim_time: False\n",
        "    alpha1: 0.2\n",
        "    alpha2: 0.2\n",
        "    alpha3: 0.2\n",
        "    alpha4: 0.2\n",
        "    alpha5: 0.2\n",
        "    base_frame_id: \"base_link\"\n",
        "    beam_skip_distance: 0.5\n",
        "    beam_skip_error_threshold: 0.9\n",
        "    beam_skip_threshold: 0.3\n",
        "    do_beamskip: false\n",
        "    global_frame_id: \"map\"\n",
        "    lambda_short: 0.1\n",
        "    laser_likelihood_max_dist: 2.0\n",
        "    laser_max_range: 100.0\n",
        "    laser_min_range: -1.0\n",
        "    laser_model_type: \"likelihood_field\"\n",
        "    max_beams: 60\n",
        "    max_particles: 2000\n",
        "    min_particles: 500\n",
        "    odom_frame_id: \"odom\"\n",
        "    pf_err: 0.05\n",
        "    pf_z: 0.99\n",
        "    recovery_alpha_fast: 0.0\n",
        "    recovery_alpha_slow: 0.0\n",
        "    resample_interval: 1\n",
        "    robot_model_type: \"nav2_amcl::DifferentialMotionModel\"\n",
        "    save_pose_rate: 0.5\n",
        "    sigma_hit: 0.2\n",
        "    tf_broadcast: true\n",
        "    transform_tolerance: 1.0\n",
        "    update_min_a: 0.2\n",
        "    update_min_d: 0.25\n",
        "    z_hit: 0.5\n",
        "    z_max: 0.05\n",
        "    z_rand: 0.5\n",
        "    z_short: 0.05\n",
        "\n",
        "bt_navigator:\n",
        "  ros__parameters:\n",
        "    use_sim_time: False\n",
        "    global_frame: map\n",
        "    robot_base_frame: base_link\n",
        "    odom_topic: /odom\n",
        "    bt_loop_duration: 10\n",
        "    default_server_timeout: 20\n",
        "    enable_groot_monitoring: True\n",
        "    groot_zmq_publisher_port: 1666\n",
        "    groot_zmq_server_port: 1667\n",
        "    plugin_lib_names:\n",
        "    - nav2_compute_path_to_pose_action_bt_node\n",
        "    - nav2_compute_path_through_poses_action_bt_node\n",
        "    - nav2_smooth_path_action_bt_node\n",
        "    - nav2_follow_path_action_bt_node\n",
        "    - nav2_spin_action_bt_node\n",
        "    - nav2_wait_action_bt_node\n",
        "    - nav2_assisted_teleop_action_bt_node\n",
        "    - nav2_back_up_action_bt_node\n",
        "    - nav2_drive_on_heading_bt_node\n",
        "    - nav2_clear_costmap_service_bt_node\n",
        "    - nav2_is_stuck_condition_bt_node\n",
        "    - nav2_goal_reached_condition_bt_node\n",
        "    - nav2_goal_updated_condition_bt_node\n",
        "    - nav2_globally_updated_goal_condition_bt_node\n",
        "    - nav2_is_path_valid_condition_bt_node\n",
        "    - nav2_initial_pose_received_condition_bt_node\n",
        "    - nav2_reinitialize_global_localization_service_bt_node\n",
        "    - nav2_rate_controller_bt_node\n",
        "    - nav2_distance_controller_bt_node\n",
        "    - nav2_speed_controller_bt_node\n",
        "    - nav2_truncate_path_action_bt_node\n",
        "    - nav2_truncate_path_local_action_bt_node\n",
        "    - nav2_goal_updater_node_bt_node\n",
        "    - nav2_recovery_node_bt_node\n",
        "    - nav2_pipeline_sequence_bt_node\n",
        "    - nav2_round_robin_node_bt_node\n",
        "    - nav2_transform_available_condition_bt_node\n",
        "    - nav2_time_expired_condition_bt_node\n",
        "    - nav2_path_expiring_timer_condition\n",
        "    - nav2_distance_traveled_condition_bt_node\n",
        "    - nav2_single_trigger_bt_node\n",
        "    - nav2_goal_updated_controller_bt_node\n",
        "    - nav2_is_battery_low_condition_bt_node\n",
        "    - nav2_navigate_through_poses_action_bt_node\n",
        "    - nav2_navigate_to_pose_action_bt_node\n",
        "    - nav2_remove_passed_goals_action_bt_node\n",
        "    - nav2_planner_selector_bt_node\n",
        "    - nav2_controller_selector_bt_node\n",
        "    - nav2_goal_checker_selector_bt_node\n",
        "    - nav2_controller_cancel_bt_node\n",
        "    - nav2_path_longer_on_approach_bt_node\n",
        "    - nav2_wait_cancel_bt_node\n",
        "    - nav2_spin_cancel_bt_node\n",
        "    - nav2_back_up_cancel_bt_node\n",
        "    - nav2_assisted_teleop_cancel_bt_node\n",
        "    - nav2_drive_on_heading_cancel_bt_node\n",
        "    - nav2_is_battery_charging_condition_bt_node\n",
        "\n",
        "controller_server:\n",
        "  ros__parameters:\n",
        "    use_sim_time: False\n",
        "    controller_frequency: 20.0\n",
        "    min_x_velocity_threshold: 0.001\n",
        "    min_y_velocity_threshold: 0.5\n",
        "    min_theta_velocity_threshold: 0.001\n",
        "    failure_tolerance: 0.3\n",
        "    progress_checker_plugin: \"progress_checker\"\n",
        "    goal_checker_plugins: [\"general_goal_checker\"]\n",
        "    controller_plugins: [\"FollowPath\"]\n",
        "\n",
        "    progress_checker:\n",
        "      plugin: \"nav2_controller::SimpleProgressChecker\"\n",
        "      required_movement_radius: 0.5\n",
        "      movement_time_allowance: 10.0\n",
        "\n",
        "    general_goal_checker:\n",
        "      stateful: True\n",
        "      plugin: \"nav2_controller::SimpleGoalChecker\"\n",
        "      xy_goal_tolerance: 0.25\n",
        "      yaw_goal_tolerance: 0.25\n",
        "\n",
        "    FollowPath:\n",
        "      plugin: \"dwb_core::DWBLocalPlanner\"\n",
        "      debug_trajectory_details: True\n",
        "      min_vel_x: 0.0\n",
        "      min_vel_y: 0.0\n",
        "      max_vel_x: 0.26\n",
        "      max_vel_y: 0.0\n",
        "      max_vel_theta: 1.0\n",
        "      min_speed_xy: 0.0\n",
        "      max_speed_xy: 0.26\n",
        "      min_speed_theta: 0.0\n",
        "      acc_lim_x: 2.5\n",
        "      acc_lim_y: 0.0\n",
        "      acc_lim_theta: 3.2\n",
        "      decel_lim_x: -2.5\n",
        "      decel_lim_y: 0.0\n",
        "      decel_lim_theta: -3.2\n",
        "      vx_samples: 20\n",
        "      vy_samples: 5\n",
        "      vtheta_samples: 20\n",
        "      sim_time: 1.7\n",
        "      linear_granularity: 0.05\n",
        "      angular_granularity: 0.025\n",
        "      transform_tolerance: 0.2\n",
        "      xy_goal_tolerance: 0.25\n",
        "      trans_stopped_velocity: 0.25\n",
        "      short_circuit_trajectory_evaluation: True\n",
        "      stateful: True\n",
        "      critics: [\"RotateToGoal\", \"Oscillation\", \"BaseObstacle\", \"GoalAlign\", \"PathAlign\", \"PathDist\", \"GoalDist\"]\n",
        "      BaseObstacle.scale: 0.02\n",
        "      PathAlign.scale: 32.0\n",
        "      PathAlign.forward_point_distance: 0.1\n",
        "      GoalAlign.scale: 24.0\n",
        "      GoalAlign.forward_point_distance: 0.1\n",
        "      PathDist.scale: 32.0\n",
        "      GoalDist.scale: 24.0\n",
        "      RotateToGoal.scale: 32.0\n",
        "      RotateToGoal.slowing_factor: 5.0\n",
        "      RotateToGoal.lookahead_time: -1.0\n",
        "```\n",
        "\n",
        "### 3. Conversational AI Core\n",
        "\n",
        "```python\n",
        "# ha_robot_ai/src/conversational_ai_node.py\n",
        "#!/usr/bin/env python3\n",
        "\n",
        "import rclpy\n",
        "from rclpy.node import Node\n",
        "from std_msgs.msg import String\n",
        "from geometry_msgs.msg import PoseStamped\n",
        "from nav2_msgs.action import NavigateToPose\n",
        "from rclpy.action import ActionClient\n",
        "import json\n",
        "import openai\n",
        "import threading\n",
        "import time\n",
        "\n",
        "class ConversationalAI(Node):\n",
        "    def __init__(self):\n",
        "        super().__init__('conversational_ai')\n",
        "\n",
        "        # Publishers\n",
        "        self.speech_pub = self.create_publisher(String, 'robot_speech', 10)\n",
        "        self.smart_home_pub = self.create_publisher(String, 'smart_home_command', 10)\n",
        "\n",
        "        # Subscribers\n",
        "        self.voice_sub = self.create_subscription(String, 'voice_input', self.voice_callback, 10)\n",
        "        self.vision_sub = self.create_subscription(String, 'vision_objects', self.vision_callback, 10)\n",
        "\n",
        "        # Action clients\n",
        "        self.navigate_client = ActionClient(self, NavigateToPose, 'navigate_to_pose')\n",
        "\n",
        "        # AI Configuration\n",
        "        self.openai_client = openai.OpenAI()  # Configure with your API key\n",
        "\n",
        "        # Robot state\n",
        "        self.current_location = \"living_room\"\n",
        "        self.known_locations = {\n",
        "            \"kitchen\": {\"x\": 2.0, \"y\": 1.0, \"z\": 0.0},\n",
        "            \"living_room\": {\"x\": 0.0, \"y\": 0.0, \"z\": 0.0},\n",
        "            \"bedroom\": {\"x\": -2.0, \"y\": 2.0, \"z\": 0.0},\n",
        "            \"bathroom\": {\"x\": 1.0, \"y\": -1.5, \"z\": 0.0}\n",
        "        }\n",
        "\n",
        "        self.conversation_history = []\n",
        "        self.detected_objects = []\n",
        "\n",
        "        self.get_logger().info(\"Conversational AI node initialized\")\n",
        "\n",
        "    def voice_callback(self, msg):\n",
        "        \"\"\"Process voice input and generate appropriate response\"\"\"\n",
        "        user_input = msg.data\n",
        "        self.get_logger().info(f\"Received voice input: {user_input}\")\n",
        "\n",
        "        # Add to conversation history\n",
        "        self.conversation_history.append({\"role\": \"user\", \"content\": user_input})\n",
        "\n",
        "        # Process with AI\n",
        "        threading.Thread(target=self.process_conversation, args=(user_input,)).start()\n",
        "\n",
        "    def vision_callback(self, msg):\n",
        "        \"\"\"Update detected objects from vision system\"\"\"\n",
        "        try:\n",
        "            self.detected_objects = json.loads(msg.data)\n",
        "        except json.JSONDecodeError:\n",
        "            self.get_logger().warn(\"Invalid JSON in vision message\")\n",
        "\n",
        "    def process_conversation(self, user_input):\n",
        "        \"\"\"Main conversation processing with AI\"\"\"\n",
        "        try:\n",
        "            # Create system prompt with robot capabilities\n",
        "            system_prompt = self.get_system_prompt()\n",
        "\n",
        "            # Prepare conversation for AI\n",
        "            messages = [{\"role\": \"system\", \"content\": system_prompt}]\n",
        "            messages.extend(self.conversation_history[-10:])  # Keep last 10 exchanges\n",
        "\n",
        "            # Get AI response\n",
        "            response = self.openai_client.chat.completions.create(\n",
        "                model=\"gpt-4\",\n",
        "                messages=messages,\n",
        "                temperature=0.7,\n",
        "                max_tokens=500\n",
        "            )\n",
        "\n",
        "            ai_response = response.choices[0].message.content\n",
        "\n",
        "            # Parse response for actions\n",
        "            self.parse_and_execute_response(ai_response)\n",
        "\n",
        "            # Add AI response to history\n",
        "            self.conversation_history.append({\"role\": \"assistant\", \"content\": ai_response})\n",
        "\n",
        "        except Exception as e:\n",
        "            self.get_logger().error(f\"Error in conversation processing: {str(e)}\")\n",
        "            self.speak(\"I'm sorry, I encountered an error processing your request.\")\n",
        "\n",
        "    def get_system_prompt(self):\n",
        "        \"\"\"Generate system prompt with current robot state\"\"\"\n",
        "        return f\"\"\"You are an intelligent home assistant robot with the following capabilities:\n",
        "\n",
        "NAVIGATION: You can move to these locations: {list(self.known_locations.keys())}\n",
        "SMART HOME: You can control lights, temperature, music, and other IoT devices\n",
        "VISION: You can see and identify objects in your environment\n",
        "CURRENT LOCATION: {self.current_location}\n",
        "DETECTED OBJECTS: {self.detected_objects}\n",
        "\n",
        "RESPONSE FORMAT:\n",
        "Always respond in a conversational manner. If you need to take actions, include them as JSON in your response:\n",
        "\n",
        "For navigation: {{\"action\": \"navigate\", \"location\": \"kitchen\"}}\n",
        "For smart home: {{\"action\": \"smart_home\", \"device\": \"lights\", \"command\": \"turn_on\", \"room\": \"living_room\"}}\n",
        "For speech only: Just respond naturally without JSON.\n",
        "\n",
        "Be helpful, friendly, and proactive in assisting with home tasks.\"\"\"\n",
        "\n",
        "    def parse_and_execute_response(self, response):\n",
        "        \"\"\"Parse AI response and execute any actions\"\"\"\n",
        "        try:\n",
        "            # Extract JSON actions from response\n",
        "            import re\n",
        "            json_matches = re.findall(r'\\{[^}]+\\}', response)\n",
        "\n",
        "            for json_str in json_matches:\n",
        "                try:\n",
        "                    action = json.loads(json_str)\n",
        "                    self.execute_action(action)\n",
        "                except json.JSONDecodeError:\n",
        "                    continue\n",
        "\n",
        "            # Always speak the response (remove JSON parts)\n",
        "            clean_response = re.sub(r'\\{[^}]+\\}', '', response).strip()\n",
        "            if clean_response:\n",
        "                self.speak(clean_response)\n",
        "\n",
        "        except Exception as e:\n",
        "            self.get_logger().error(f\"Error parsing response: {str(e)}\")\n",
        "\n",
        "    def execute_action(self, action):\n",
        "        \"\"\"Execute specific actions based on AI response\"\"\"\n",
        "        action_type = action.get(\"action\")\n",
        "\n",
        "        if action_type == \"navigate\":\n",
        "            location = action.get(\"location\")\n",
        "            if location in self.known_locations:\n",
        "                self.navigate_to_location(location)\n",
        "            else:\n",
        "                self.speak(f\"I don't know how to get to {location}\")\n",
        "\n",
        "        elif action_type == \"smart_home\":\n",
        "            # Publish smart home command\n",
        "            command_msg = String()\n",
        "            command_msg.data = json.dumps(action)\n",
        "            self.smart_home_pub.publish(command_msg)\n",
        "\n",
        "    def navigate_to_location(self, location):\n",
        "        \"\"\"Navigate to a specific location\"\"\"\n",
        "        if location not in self.known_locations:\n",
        "            self.get_logger().error(f\"Unknown location: {location}\")\n",
        "            return\n",
        "\n",
        "        # Create navigation goal\n",
        "        goal_msg = NavigateToPose.Goal()\n",
        "        goal_msg.pose.header.frame_id = \"map\"\n",
        "        goal_msg.pose.header.stamp = self.get_clock().now().to_msg()\n",
        "\n",
        "        coords = self.known_locations[location]\n",
        "        goal_msg.pose.pose.position.x = coords[\"x\"]\n",
        "        goal_msg.pose.pose.position.y = coords[\"y\"]\n",
        "        goal_msg.pose.pose.position.z = coords[\"z\"]\n",
        "        goal_msg.pose.pose.orientation.w = 1.0\n",
        "\n",
        "        # Send goal\n",
        "        self.navigate_client.wait_for_server()\n",
        "        send_goal_future = self.navigate_client.send_goal_async(goal_msg)\n",
        "\n",
        "        self.speak(f\"Navigating to {location}\")\n",
        "        self.current_location = location\n",
        "\n",
        "    def speak(self, text):\n",
        "        \"\"\"Publish text for speech synthesis\"\"\"\n",
        "        msg = String()\n",
        "        msg.data = text\n",
        "        self.speech_pub.publish(msg)\n",
        "\n",
        "def main(args=None):\n",
        "    rclpy.init(args=args)\n",
        "    node = ConversationalAI()\n",
        "\n",
        "    try:\n",
        "        rclpy.spin(node)\n",
        "    except KeyboardInterrupt:\n",
        "        pass\n",
        "    finally:\n",
        "        node.destroy_node()\n",
        "        rclpy.shutdown()\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n",
        "```\n",
        "\n",
        "### 4. Voice Interface\n",
        "\n",
        "```python\n",
        "# ha_robot_voice/src/voice_interface_node.py\n",
        "#!/usr/bin/env python3\n",
        "\n",
        "import rclpy\n",
        "from rclpy.node import Node\n",
        "from std_msgs.msg import String\n",
        "import speech_recognition as sr\n",
        "import pyttsx3\n",
        "import threading\n",
        "import queue\n",
        "import time\n",
        "\n",
        "class VoiceInterface(Node):\n",
        "    def __init__(self):\n",
        "        super().__init__('voice_interface')\n",
        "\n",
        "        # Publishers\n",
        "        self.voice_pub = self.create_publisher(String, 'voice_input', 10)\n",
        "\n",
        "        # Subscribers\n",
        "        self.speech_sub = self.create_subscription(String, 'robot_speech', self.speak_callback, 10)\n",
        "\n",
        "        # Speech recognition setup\n",
        "        self.recognizer = sr.Recognizer()\n",
        "        self.microphone = sr.Microphone()\n",
        "\n",
        "        # Text-to-speech setup\n",
        "        self.tts_engine = pyttsx3.init()\n",
        "        self.tts_engine.setProperty('rate', 150)\n",
        "        self.tts_engine.setProperty('volume', 0.8)\n",
        "\n",
        "        # Speech queue and flags\n",
        "        self.speech_queue = queue.Queue()\n",
        "        self.is_speaking = False\n",
        "        self.listening = True\n",
        "\n",
        "        # Wake words\n",
        "        self.wake_words = [\"robot\", \"assistant\", \"hey robot\"]\n",
        "\n",
        "        # Calibrate microphone\n",
        "        with self.microphone as source:\n",
        "            self.recognizer.adjust_for_ambient_noise(source)\n",
        "\n",
        "        # Start threads\n",
        "        threading.Thread(target=self.listen_continuous, daemon=True).start()\n",
        "        threading.Thread(target=self.speech_worker, daemon=True).start()\n",
        "\n",
        "        self.get_logger().info(\"Voice interface initialized. Say 'robot' or 'hey robot' to wake me up.\")\n",
        "\n",
        "    def listen_continuous(self):\n",
        "        \"\"\"Continuously listen for voice commands\"\"\"\n",
        "        while rclpy.ok():\n",
        "            try:\n",
        "                with self.microphone as source:\n",
        "                    # Listen for audio with timeout\n",
        "                    audio = self.recognizer.listen(source, timeout=1, phrase_time_limit=5)\n",
        "\n",
        "                try:\n",
        "                    # Recognize speech\n",
        "                    text = self.recognizer.recognize_google(audio).lower()\n",
        "                    self.get_logger().info(f\"Heard: {text}\")\n",
        "\n",
        "                    # Check for wake words\n",
        "                    if any(wake_word in text for wake_word in self.wake_words):\n",
        "                        # Remove wake word and process command\n",
        "                        command = text\n",
        "                        for wake_word in self.wake_words:\n",
        "                            command = command.replace(wake_word, \"\").strip()\n",
        "\n",
        "                        if command:\n",
        "                            self.process_voice_command(command)\n",
        "                        else:\n",
        "                            self.add_to_speech_queue(\"Yes, how can I help you?\")\n",
        "\n",
        "                except sr.UnknownValueError:\n",
        "                    # Speech was unintelligible\n",
        "                    pass\n",
        "                except sr.RequestError as e:\n",
        "                    self.get_logger().error(f\"Could not request results; {e}\")\n",
        "\n",
        "            except sr.WaitTimeoutError:\n",
        "                # No speech detected, continue listening\n",
        "                pass\n",
        "            except Exception as e:\n",
        "                self.get_logger().error(f\"Error in speech recognition: {str(e)}\")\n",
        "                time.sleep(1)\n",
        "\n",
        "    def process_voice_command(self, command):\n",
        "        \"\"\"Process recognized voice command\"\"\"\n",
        "        if not command.strip():\n",
        "            return\n",
        "\n",
        "        self.get_logger().info(f\"Processing command: {command}\")\n",
        "\n",
        "        # Publish voice command\n",
        "        msg = String()\n",
        "        msg.data = command\n",
        "        self.voice_pub.publish(msg)\n",
        "\n",
        "    def speak_callback(self, msg):\n",
        "        \"\"\"Handle text-to-speech requests\"\"\"\n",
        "        text = msg.data\n",
        "        self.add_to_speech_queue(text)\n",
        "\n",
        "    def add_to_speech_queue(self, text):\n",
        "        \"\"\"Add text to speech queue\"\"\"\n",
        "        self.speech_queue.put(text)\n",
        "\n",
        "    def speech_worker(self):\n",
        "        \"\"\"Worker thread for text-to-speech\"\"\"\n",
        "        while rclpy.ok():\n",
        "            try:\n",
        "                # Get text from queue (blocking)\n",
        "                text = self.speech_queue.get(timeout=1)\n",
        "\n",
        "                if text:\n",
        "                    self.is_speaking = True\n",
        "                    self.get_logger().info(f\"Speaking: {text}\")\n",
        "\n",
        "                    # Use TTS to speak\n",
        "                    self.tts_engine.say(text)\n",
        "                    self.tts_engine.runAndWait()\n",
        "\n",
        "                    self.is_speaking = False\n",
        "\n",
        "            except queue.Empty:\n",
        "                continue\n",
        "            except Exception as e:\n",
        "                self.get_logger().error(f\"Error in speech synthesis: {str(e)}\")\n",
        "                self.is_speaking = False\n",
        "\n",
        "def main(args=None):\n",
        "    rclpy.init(args=args)\n",
        "    node = VoiceInterface()\n",
        "\n",
        "    try:\n",
        "        rclpy.spin(node)\n",
        "    except KeyboardInterrupt:\n",
        "        pass\n",
        "    finally:\n",
        "        node.destroy_node()\n",
        "        rclpy.shutdown()\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n",
        "```\n",
        "\n",
        "### 5. Smart Home Integration\n",
        "\n",
        "```python\n",
        "# ha_robot_smart_home/src/smart_home_node.py\n",
        "#!/usr/bin/env python3\n",
        "\n",
        "import rclpy\n",
        "from rclpy.node import Node\n",
        "from std_msgs.msg import String\n",
        "import json\n",
        "import requests\n",
        "import paho.mqtt.client as mqtt\n",
        "from typing import Dict, Any\n",
        "\n",
        "class SmartHomeController(Node):\n",
        "    def __init__(self):\n",
        "        super().__init__('smart_home_controller')\n",
        "\n",
        "        # Subscribers\n",
        "        self.command_sub = self.create_subscription(\n",
        "            String, 'smart_home_command', self.command_callback, 10)\n",
        "\n",
        "        # Publishers\n",
        "        self.status_pub = self.create_publisher(String, 'smart_home_status', 10)\n",
        "\n",
        "        # Device configurations\n",
        "        self.devices = {\n",
        "            \"lights\": {\n",
        "                \"living_room\": {\"type\": \"philips_hue\", \"id\": \"1\"},\n",
        "                \"kitchen\": {\"type\": \"philips_hue\", \"id\": \"2\"},\n",
        "                \"bedroom\": {\"type\": \"philips_hue\", \"id\": \"3\"},\n",
        "                \"bathroom\": {\"type\": \"philips_hue\", \"id\": \"4\"}\n",
        "            },\n",
        "            \"thermostat\": {\n",
        "                \"main\": {\"type\": \"nest\", \"id\": \"thermostat_1\"}\n",
        "            },\n",
        "            \"music\": {\n",
        "                \"speakers\": {\"type\": \"spotify\", \"device_id\": \"main_speakers\"}\n",
        "            },\n",
        "            \"tv\": {\n",
        "                \"living_room\": {\"type\": \"samsung\", \"ip\": \"192.168.1.100\"}\n",
        "            }\n",
        "        }\n",
        "\n",
        "        # API configurations (load from config file)\n",
        "        self.philips_hue_bridge = \"192.168.1.50\"\n",
        "        self.philips_hue_token = \"your_hue_token_here\"\n",
        "        self.nest_token = \"your_nest_token_here\"\n",
        "        self.spotify_token = \"your_spotify_token_here\"\n",
        "\n",
        "        # MQTT setup for other IoT devices\n",
        "        self.mqtt_client = mqtt.Client()\n",
        "        self.mqtt_client.on_connect = self.on_mqtt_connect\n",
        "        self.mqtt_client.on_message = self.on_mqtt_message\n",
        "\n",
        "        try:\n",
        "            self.mqtt_client.connect(\"localhost\", 1883, 60)\n",
        "            self.mqtt_client.loop_start()\n",
        "        except Exception as e:\n",
        "            self.get_logger().warn(f\"Could not connect to MQTT broker: {e}\")\n",
        "\n",
        "        self.get_logger().info(\"Smart Home Controller initialized\")\n",
        "\n",
        "    def command_callback(self, msg):\n",
        "        \"\"\"Process smart home commands\"\"\"\n",
        "        try:\n",
        "            command = json.loads(msg.data)\n",
        "            self.get_logger().info(f\"Received smart home command: {command}\")\n",
        "\n",
        "            device_type = command.get(\"device\")\n",
        "            action = command.get(\"command\")\n",
        "            room = command.get(\"room\", \"main\")\n",
        "\n",
        "            if device_type == \"lights\":\n",
        "                self.control_lights(action, room, command)\n",
        "            elif device_type == \"thermostat\":\n",
        "                self.control_thermostat(action, command)\n",
        "            elif device_type == \"music\":\n",
        "                self.control_music(action, command)\n",
        "            elif device_type == \"tv\":\n",
        "                self.control_tv(action, room, command)\n",
        "            else:\n",
        "                self.get_logger().warn(f\"Unknown device type: {device_type}\")\n",
        "\n",
        "        except json.JSONDecodeError:\n",
        "            self.get_logger().error(\"Invalid JSON in smart home command\")\n",
        "        except Exception as e:\n",
        "            self.get_logger().error(f\"Error processing smart home command: {str(e)}\")\n",
        "\n",
        "    def control_lights(self, action: str, room: str, command: Dict[str, Any]):\n",
        "        \"\"\"Control Philips Hue lights\"\"\"\n",
        "        if room not in self.devices[\"lights\"]:\n",
        "            self.get_logger().warn(f\"No lights configured for room: {room}\")\n",
        "            return\n",
        "\n",
        "        light_id = self.devices[\"lights\"][room][\"id\"]\n",
        "\n",
        "        try:\n",
        "            if action == \"turn_on\":\n",
        "                payload = {\"on\": True}\n",
        "                if \"brightness\" in command:\n",
        "                    payload[\"bri\"] = int(command[\"brightness\"] * 2.54)  # 0-255 scale\n",
        "                if \"color\" in command:\n",
        "                    # Convert color name to hue value\n",
        "                    color_map = {"
      ]
    }
  ]
}